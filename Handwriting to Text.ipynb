{"cells":[{"cell_type":"code","execution_count":null,"id":"5db341ad-6bc0-415e-b54b-3c7ea5c647b8","metadata":{"id":"5db341ad-6bc0-415e-b54b-3c7ea5c647b8"},"outputs":[],"source":["%matplotlib inline\n","import os\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n","import random\n","import numpy as np\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","import matplotlib.pyplot as plt\n","import cv2\n","import time\n","from PIL import Image, ImageEnhance"]},{"cell_type":"code","execution_count":null,"id":"452e48be-976b-4ed9-8b72-1c611a9e2d7d","metadata":{"id":"452e48be-976b-4ed9-8b72-1c611a9e2d7d"},"outputs":[],"source":["# Constants\n","vocab_set = set()\n","max_label_length = 0\n","vocab_size = 53 # change this\n","input_size = 512\n","output_size = vocab_size\n","hidden_size = 256\n","batch_size = 64\n","dropout_rate = 0.1\n","epochs = 5 # change this to >=100 after testing a little\n","learning_rate = 0.01\n","in_channels = 1 # need to define this\n","height = 32\n","width = 128\n","\n","# Checked the vocab:\n","# i|L|0|M|K|/|U|O|Y|s|F|N|S|W|:|D|2|-|P|4|9|j|r|J|(|;|?|Z|1|#|B|8|7|G|h|e|n|!|f|A|C|a|6|.|R|d| |q|V|H|m|z|I|l|E|w|v|5|\"|,|T|)|Q|k|b|X|x|u|'|y|c|3|g|p|t|*|o|\n","# 77\n"]},{"cell_type":"code","execution_count":null,"id":"2d0fca9d-8f43-45f5-b2f7-6a3ba16cf310","metadata":{"id":"2d0fca9d-8f43-45f5-b2f7-6a3ba16cf310"},"outputs":[],"source":["import kagglehub\n","path = kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\")"]},{"cell_type":"code","source":["# https://www.geeksforgeeks.org/image-resizing-using-opencv-python/\n","# width, height\n","\n","path = \"/root/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/\"\n","for root, dirs, files in os.walk(path):\n","    print(f\"Root: {root}, Files: {files}\")"],"metadata":{"id":"cof0s7sPq4JU","collapsed":true,"executionInfo":{"status":"ok","timestamp":1733548990386,"user_tz":480,"elapsed":1915,"user":{"displayName":"James Hu","userId":"14968480125287557184"}},"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XV-CL4NNNpBUimtUH3lPCOocmySqNh9Y"},"outputId":"72493cd8-de39-4384-8367-44f1a951b1a9"},"id":"cof0s7sPq4JU","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","execution_count":null,"id":"5443c6da-d38c-41c7-8678-02b15c6ce6c3","metadata":{"id":"5443c6da-d38c-41c7-8678-02b15c6ce6c3","executionInfo":{"status":"ok","timestamp":1733549128831,"user_tz":480,"elapsed":138448,"user":{"displayName":"James Hu","userId":"14968480125287557184"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02f96487-0636-4a97-9a0a-2fe1eebc5e18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image processing error, return none\n"]}],"source":["def gray_scale_check(inputPath, outputPath):\n","  try:\n","    image = Image.open(inputPath)\n","    if image.mode == \"RGB\": #only convert if RBG\n","        pixels = image.load()\n","        width, height = image.size\n","        grayscaleImage = Image.new(\"L\", (width, height))\n","        grayscalePixels = grayscaleImage.load()\n","        for x in range(width):\n","            for y in range(height):\n","                r, g, b = pixels[x, y]\n","                grayValue = int(0.299 * r + 0.587 * g + 0.114 * b) #grayscale formula\n","                grayscalePixels[x, y] = grayValue\n","        grayscaleImage.save(outputPath)\n","        return grayscaleImage\n","    else:\n","        image.save(outputPath)\n","        return image\n","  except(OSError, Image.UnidentifiedImageError) as e:\n","      print(f\"Image processing error, return none\")\n","      return None #these images will be excluded from the dataset\n","\n","\n","def label_augment_preprocess():\n","    datasetRoot = \"/root/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/\"\n","    wordsNewPath = os.path.join(datasetRoot, \"words_new.txt\")\n","    wPath = os.path.join(datasetRoot, \"iam_words/words.txt\")\n","    iamWordsDir = os.path.join(datasetRoot, \"iam_words/words\")\n","    outputDir = \"/content/labeledImages\"\n","    inputFolder = \"/content/labeledImages\"\n","    preprocessedFolder = \"/content/preprocessed\"\n","\n","    Labels = {}\n","    with open(wordsNewPath, \"r\") as f:\n","        lines = f.readlines()\n","    for line in lines:\n","        if line.startswith(\"#\") or len(line.strip()) == 0: continue\n","        cols = line.split()\n","        if cols[1] == \"err\": continue #just skip if theres an error\n","        fileId = cols[0]\n","        transcription = \" \".join(cols[8:])\n","        Labels[fileId] = transcription\n","    os.makedirs(preprocessedFolder, exist_ok=True)\n","\n","    for root, _, files in os.walk(iamWordsDir):\n","        for filename in files:\n","            if filename.endswith(('.png', '.jpg', '.jpeg')): #valid etension\n","                fileId = filename.rsplit('.', 1)[0]\n","                if fileId in Labels:\n","                    label = Labels[fileId]\n","                    labelDir = os.path.join(outputDir, label)\n","                    global max_label_length\n","                    if len(label) > max_label_length:\n","                        max_label_length = len(label)\n","                    for char in label: # The block is to adjust the vocab size\n","                      vocab_set.add(char)\n","\n","                    os.makedirs(labelDir, exist_ok=True)\n","                    inputPath = os.path.join(root, filename)\n","                    outputPath = os.path.join(labelDir, filename)\n","\n","                    with open(inputPath, 'rb') as srcFile, open(outputPath, 'wb') as destFile:\n","                        destFile.write(srcFile.read()) #write label to file\n","\n","    for root, _, files in os.walk(inputFolder):\n","        for filename in files:\n","            if filename.endswith(('.png', '.jpg', '.jpeg')):\n","                inputPath = os.path.join(root, filename)\n","                relativePath = os.path.relpath(inputPath, inputFolder)\n","                outputPath = os.path.join(preprocessedFolder, relativePath)\n","\n","                os.makedirs(os.path.dirname(outputPath), exist_ok=True)\n","                grayscaleImage = gray_scale_check(inputPath, outputPath)\n","\n","                if grayscaleImage is None:\n","                  continue #exclude image from handling\n","\n","                cropWidth, cropHeight = (10, 10) #crop using small numbers because the images of letters are very small\n","                imgWidth, imgHeight = grayscaleImage.size\n","                croppedImage = None\n","                if imgWidth >= cropWidth and imgHeight >= cropHeight:\n","                    top = random.randint(0, imgHeight - cropHeight)\n","                    left = random.randint(0, imgWidth - cropWidth)\n","                    croppedImage = grayscaleImage.crop((left, top, left + cropWidth, top + cropHeight))\n","                    #only crops if it is large enough to be cropped\n","                bFactor, cFactor = 1.2, 0.8 #small enough to be recognizable but still augment. can play around with these numbers\n","\n","                brightnessAdjusted = (ImageEnhance.Brightness(grayscaleImage)).enhance(bFactor)\n","                contrastAdjusted = ImageEnhance.Contrast(brightnessAdjusted).enhance(cFactor)\n","                if croppedImage:\n","                    croppedOutputPath = outputPath.replace('.png', '_crop.png').replace('.jpg', '_crop.jpg')\n","                    croppedImage.save(croppedOutputPath) #finally save augmented images\n","                contrastAdjustedOutputPath = outputPath.replace('.png', '_bc.png').replace('.jpg', '_bc.jpg')\n","\n","                contrastAdjusted.save(contrastAdjustedOutputPath) # ^^\n","\n","label_augment_preprocess()"]},{"cell_type":"code","source":["# Instantiate a dataset class\n","class HandwritingDataset(Dataset):\n","    def __init__(self, preprocessedFolder, vocab, transform=None):\n","        self.data_dir = preprocessedFolder\n","        self.transform = transform\n","        self.vocab = vocab\n","        self.image_paths = []\n","        self.labels = []\n","\n","        for root, _, files in os.walk(self.data_dir):\n","          for file in files:\n","            if file.endswith(('.png', '.jpg', '.jpeg')):\n","              self.image_paths.append(os.path.join(root, file))\n","              self.labels.append(os.path.basename(root))\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","      image_path = self.image_paths[idx]\n","      label = self.labels[idx]\n","      image = Image.open(image_path)\n","\n","      # transform (resizing and conversion to tensor)\n","      if self.transform:\n","            image = self.transform(image)\n","\n","      # convert label into a tensor and pad it to the max length\n","      label = torch.tensor([self.vocab[char] for char in label], dtype = torch.long)\n","\n","      padding_length = max_label_length - label.size(0)\n","      if padding_length > 0:\n","            label = torch.cat([label, torch.zeros(padding_length, dtype=torch.long)])\n","\n","      return image, label\n","\n","# Initialize folders and the datasets\n","preprocessedFolder = \"/content/preprocessed\"\n","nonprocessedFolder = \"/content/labeledImages\"\n","\n","vocab = {char: idx for idx, char in enumerate(sorted(vocab_set))}\n","\n","transform = transforms.Compose([transforms.Resize((32, 128)),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,))])\n","\n","trainset = HandwritingDataset(preprocessedFolder, vocab, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=True, num_workers = 2)\n","\n","testset = HandwritingDataset(nonprocessedFolder, vocab, transform = transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle=False, num_workers = 2)"],"metadata":{"id":"gwn5UcpOJIgo"},"id":"gwn5UcpOJIgo","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9cfabf60-9fb9-412b-92d3-d937805b0d42","metadata":{"id":"9cfabf60-9fb9-412b-92d3-d937805b0d42"},"outputs":[],"source":["# cnn\n","class testCNN(nn.Module):\n","  def __init__(self, in_channels: int, out_channels: int):\n","    super(testCNN, self).__init__()\n","    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","    self.bn1 = nn.BatchNorm2d(out_channels)\n","\n","    self.conv2 = nn.Conv2d(out_channels, 64, kernel_size=3, stride=1, padding=1)\n","    self.bn2 = nn.BatchNorm2d(64)\n","\n","    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","    self.bn3 = nn.BatchNorm2d(128)\n","\n","    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","    self.bn4 = nn.BatchNorm2d(256)\n","\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    # could change droupout\n","    self.dropout = nn.Dropout(0.3)\n","\n","    #self.fc = nn.Linear(256 * 8 * 8, 512)\n","\n","  def forward(self, x):\n","    x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","    x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","    x = self.dropout(x)\n","    x = self.pool(F.relu(self.bn3(self.conv3(x))))\n","    x = self.dropout(x)\n","    x = self.pool(F.relu(self.bn4(self.conv4(x))))\n","    x = self.dropout(x)\n","\n","    return x"]},{"cell_type":"code","execution_count":null,"id":"a00cb2a1-5207-4b91-aab9-1ff0e7380600","metadata":{"id":"a00cb2a1-5207-4b91-aab9-1ff0e7380600"},"outputs":[],"source":["# 2 LSTM layers\n","class BidirectionalLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n","        super().__init__()\n","        self.cnnLayer = testCNN(1, 32)\n","        # (batch_size, 256, w/16, h/16) ==flatten=> (batch_size, 256*h/16*w/16)\n","        # lstm format is (batch_size, sequence_length, input_size)\n","        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 2, bias = True, batch_first = False, dropout = dropout_rate, bidirectional = True)\n","        self.fc = nn.Linear(hidden_size * 2, output_size)\n","\n","    def forward(self, x):\n","        x = self.cnnLayer(x)\n","        # reshape output from cnn\n","        x = x.permute(0, 3, 1, 2).contiguous()\n","        x = x.view(x.size(0), x.size(1), -1)\n","        x, _ = self.lstm(x)\n","        x = self.fc(x)\n","        x = torch.softmax(x, dim = 2)\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"3a29684f-de2e-42b1-9416-a736bebebd21","metadata":{"id":"3a29684f-de2e-42b1-9416-a736bebebd21"},"outputs":[],"source":["# Loss and optimizers\n","model = BidirectionalLSTM(input_size = input_size, hidden_size = hidden_size, output_size = output_size, dropout_rate = dropout_rate)\n","\n","criterion = nn.CTCLoss(blank = 0, zero_infinity = True)\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)"]},{"cell_type":"code","execution_count":null,"id":"18c20e1b-888a-4108-91b6-984d51ab2212","metadata":{"id":"18c20e1b-888a-4108-91b6-984d51ab2212"},"outputs":[],"source":["# Lists to store metrics for plotting\n","loss_metric = []\n","training_accuracy = []\n","test_accuracy = []"]},{"cell_type":"code","execution_count":null,"id":"622355ad-8520-45a1-b233-fa6ebdcf93d6","metadata":{"id":"622355ad-8520-45a1-b233-fa6ebdcf93d6","executionInfo":{"status":"error","timestamp":1733550836469,"user_tz":480,"elapsed":2208,"user":{"displayName":"James Hu","userId":"14968480125287557184"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4604efaf-cebe-4af6-f99d-592dcab1dac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 1, 32, 128])\n","torch.Size([64, 19])\n","torch.Size([64])\n","torch.Size([64])\n","64\n","tensor([51, 68, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([51, 70,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([53, 58, 51, 62, 62, 55, 64, 57, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([53, 51, 69, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([43, 58, 55, 59, 62, 51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([54, 55, 52, 51, 70, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([32, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([58, 51, 64, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([33, 70,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([73, 58, 55, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([73, 58, 65, 69, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([54, 65,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([62, 59, 53, 61, 59, 64, 57,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([58, 59, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([66, 68, 55, 66, 68, 65, 53, 55, 69, 69, 55, 54,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 58, 51, 70,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([73, 65, 71, 62, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([56, 62, 71, 55, 64, 53, 55, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 58, 51, 70,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 58, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([51, 64, 53, 58, 65, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([68, 55, 56, 55, 68, 55, 64, 53, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([72, 59, 55, 73,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([66, 68, 55, 53, 59, 65, 71, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([52, 55, 70, 73, 55, 55, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([55, 53, 65, 64, 65, 63, 59, 53,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([62, 59, 72, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([53, 65, 64, 53, 59, 62, 59, 51, 70, 65, 68, 69,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 65,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([51, 53, 70, 59, 65, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([65, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([52, 65, 75, 53, 65, 70, 70, 59, 64, 57,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([65, 56,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([73, 58, 59, 53, 58,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([59, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([63, 51, 54, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([69, 58, 65, 71, 62, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([52, 55, 59, 64, 57,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([59, 62, 62, 64, 55, 69, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([73, 65, 63, 55, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([59, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 58, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 58, 55, 75,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([52, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([53, 65, 64, 56, 55, 68, 55, 64, 53, 55,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([56, 59, 62, 63,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([54, 55, 72, 55, 62, 65, 66, 63, 55, 64, 70,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([44, 58, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([51, 62, 70, 58, 65, 71, 57, 58,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([72, 55, 68, 69, 59, 65, 64,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([54, 55, 69, 70, 68, 71, 53, 70, 59, 72, 55,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([27, 65, 63, 63, 59, 69, 69, 59, 65, 64, 55, 68,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([68, 55, 53, 65, 63, 63, 55, 64, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([70, 58, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([59, 54, 55, 51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([29, 64, 57, 62, 51, 64, 54,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([58, 51, 72, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","tensor([52, 55,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([31, 65, 72, 55, 68, 64, 63, 55, 64, 70,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([58, 59, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n","tensor([56, 65, 68, 63,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"input_lengths must be of size batch_size","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-42c586c8ee26>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m     ) -> Tensor:\n\u001b[0;32m-> 1980\u001b[0;31m         return F.ctc_loss(\n\u001b[0m\u001b[1;32m   1981\u001b[0m             \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m         )\n\u001b[0;32m-> 3069\u001b[0;31m     return torch.ctc_loss(\n\u001b[0m\u001b[1;32m   3070\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3071\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"]}],"source":["t0 = time.time()\n","for epoch in range(epochs):\n","\n","    # Statistics\n","    correct = 0\n","    total = 0\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        # introduce input and target lengths for ctc loss\n","        input_lengths = torch.full((inputs.size(0),), outputs.size(1), dtype=torch.long)\n","        target_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n","\n","        print(inputs.shape)\n","        print(labels.shape)\n","        print(input_lengths.shape)\n","        print(target_lengths.shape)\n","        print(batch_size)\n","        for i in range(batch_size):\n","            print(labels[i])\n","\n","        loss = criterion(outputs, labels, input_lengths, target_lengths)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics and calculate accuracy\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    # Record the training accuracy and loss\n","    epoch_loss = running_loss / len(trainloader)\n","    epoch_accuracy = correct / total\n","    loss_metric.append(epoch_loss)\n","    training_accuracy.append(epoch_accuracy)\n","\n","    # Record the testing accuracy\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    acc = correct / total\n","    test_accuracy.append(acc)\n","\n","print('Finished Training')\n","t1 = time.time()\n","total = t1-t0\n","print(f\"Testing took {total:.2f} seconds for {epochs} epochs\")\n","\n","# Plot loss\n","plt.subplot(1, 3, 1)\n","plt.plot(train_loss_list, label='Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss')\n","plt.grid(True)\n","\n","# Plot training accuracy\n","plt.subplot(1, 3, 2)\n","plt.plot(train_acc_list, label='Training Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.title('Training Accuracy')\n","plt.grid(True)\n","\n","# Plot test accuracy\n","plt.subplot(1, 3, 3)\n","plt.plot(test_acc_list, label='Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.title('Test Accuracy')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}