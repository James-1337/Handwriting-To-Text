{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db341ad-6bc0-415e-b54b-3c7ea5c647b8",
   "metadata": {
    "id": "5db341ad-6bc0-415e-b54b-3c7ea5c647b8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from PIL import Image, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e48be-976b-4ed9-8b72-1c611a9e2d7d",
   "metadata": {
    "id": "452e48be-976b-4ed9-8b72-1c611a9e2d7d"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "vocab_set = set()\n",
    "max_label_length = 0\n",
    "vocab_size = 0\n",
    "input_size = 512\n",
    "output_size = vocab_size\n",
    "hidden_size = 512\n",
    "batch_size = 64\n",
    "dropout_rate = 0.3\n",
    "epochs = 150\n",
    "learning_rate = 0.0008\n",
    "height = 32\n",
    "width = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fca9d-8f43-45f5-b2f7-6a3ba16cf310",
   "metadata": {
    "id": "2d0fca9d-8f43-45f5-b2f7-6a3ba16cf310"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cof0s7sPq4JU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cof0s7sPq4JU",
    "outputId": "b065a994-352f-4108-acea-be7f338491e1"
   },
   "outputs": [],
   "source": [
    "#path = \"/root/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/\"\n",
    "path = \"/Users/hufen/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/\"\n",
    "for root, dirs, files in os.walk(path):\n",
    "    print(f\"Root: {root}, Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443c6da-d38c-41c7-8678-02b15c6ce6c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5443c6da-d38c-41c7-8678-02b15c6ce6c3",
    "outputId": "864b7f31-4f0d-463c-94fe-912b4fa07d9c"
   },
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    # Replace invalid characters with an underscore\n",
    "    return re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1F]', '_', filename)\n",
    "\n",
    "def gray_scale_check(inputPath, outputPath):\n",
    "  try:\n",
    "    image = Image.open(inputPath)\n",
    "    if image.mode == \"RGB\": #only convert if RBG\n",
    "        pixels = image.load()\n",
    "        width, height = image.size\n",
    "        grayscaleImage = Image.new(\"L\", (width, height))\n",
    "        grayscalePixels = grayscaleImage.load()\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                r, g, b = pixels[x, y]\n",
    "                grayValue = int(0.299 * r + 0.587 * g + 0.114 * b) #grayscale formula\n",
    "                grayscalePixels[x, y] = grayValue\n",
    "        grayscaleImage.save(outputPath)\n",
    "        return grayscaleImage\n",
    "    else:\n",
    "        image.save(outputPath)\n",
    "        return image\n",
    "  except(OSError, Image.UnidentifiedImageError) as e:\n",
    "      print(f\"Image processing error, return none\")\n",
    "      return None #these images will be excluded from the dataset\n",
    "\n",
    "def label_augment_preprocess():\n",
    "    #datasetRoot = \"/root/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/\"\n",
    "    datasetRoot = \"/Users/hufen/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/\"\n",
    "    wordsNewPath = os.path.join(datasetRoot, \"words_new.txt\")\n",
    "    Path = os.path.join(datasetRoot, \"iam_words/words.txt\")\n",
    "    iamWordsDir = os.path.join(datasetRoot, \"iam_words/words\")\n",
    "    #outputDir = \"/content/labeledImages\"\n",
    "    #inputFolder = \"/content/labeledImages\"\n",
    "    #preprocessedFolder = \"/content/preprocessed\"\n",
    "    outputDir = \"/Users/hufen/Coding/ECS 174/Project/labeledImages\"\n",
    "    inputFolder = \"/Users/hufen/Coding/ECS 174/Project/labeledImages\"\n",
    "    preprocessedFolder = \"/Users/hufen/Coding/ECS 174/Project/preprocessed\"\n",
    "\n",
    "    Labels = {}\n",
    "    with open(wordsNewPath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith(\"#\") or len(line.strip()) == 0: continue\n",
    "        cols = line.split()\n",
    "        if cols[1] == \"err\": continue #just skip if theres an error\n",
    "        fileId = cols[0]\n",
    "        transcription = \" \".join(cols[8:])\n",
    "        Labels[fileId] = transcription\n",
    "    os.makedirs(preprocessedFolder, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(iamWordsDir):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')): #valid extension\n",
    "                fileId = filename.rsplit('.', 1)[0]\n",
    "                if fileId in Labels:\n",
    "                    label = Labels[fileId]\n",
    "                    # Sanitize the label to remove invalid characters\n",
    "                    label = sanitize_filename(label)\n",
    "                    labelDir = os.path.join(outputDir, label)\n",
    "                    global max_label_length\n",
    "                    global vocab_set\n",
    "                    if len(label) > max_label_length:\n",
    "                        max_label_length = len(label)\n",
    "                    for char in label: # The block is to adjust the vocab size\n",
    "                      vocab_set.add(char)\n",
    "\n",
    "                    os.makedirs(labelDir, exist_ok=True)\n",
    "                    inputPath = os.path.join(root, filename)\n",
    "                    outputPath = os.path.join(labelDir, filename)\n",
    "\n",
    "                    try:\n",
    "                        with open(inputPath, 'rb') as srcFile, open(outputPath, 'wb') as destFile:\n",
    "                            destFile.write(srcFile.read())\n",
    "                    except(OSError, Image.UnidentifiedImageError) as e:\n",
    "                        print(f\"Image processing error: {e}\")\n",
    "\n",
    "    for root, _, files in os.walk(inputFolder):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                inputPath = os.path.join(root, filename)\n",
    "                relativePath = os.path.relpath(inputPath, inputFolder)\n",
    "                outputPath = os.path.join(preprocessedFolder, relativePath)\n",
    "\n",
    "                os.makedirs(os.path.dirname(outputPath), exist_ok=True)\n",
    "                grayscaleImage = gray_scale_check(inputPath, outputPath)\n",
    "\n",
    "                if grayscaleImage is None:\n",
    "                  continue #exclude image from handling\n",
    "\n",
    "                imgWidth, imgHeight = grayscaleImage.size\n",
    "                cropWidth, cropHeight = int((imgWidth * 0.95)), int((imgHeight * 0.95)) #crop to 95% input height and width at most\n",
    "                croppedImage = None\n",
    "                if imgWidth >= width and imgHeight >= height:\n",
    "                    top = random.randint(0, imgHeight - cropHeight)\n",
    "                    left = random.randint(0, imgWidth - cropWidth)\n",
    "                    right = left + cropWidth\n",
    "                    bottom = top + cropHeight\n",
    "                    croppedImage = grayscaleImage.crop((left, top, right, bottom))\n",
    "                    #only crops if it is large enough to be cropped\n",
    "\n",
    "                bFactor = random.uniform(0.8, 1.2)\n",
    "                cFactor = random.uniform(0.8, 1.2)\n",
    "\n",
    "                brightnessAdjusted = (ImageEnhance.Brightness(grayscaleImage)).enhance(bFactor)\n",
    "                contrastAdjusted = ImageEnhance.Contrast(brightnessAdjusted).enhance(cFactor)\n",
    "                if croppedImage:\n",
    "                    croppedOutputPath = outputPath.replace('.png', '_crop.png').replace('.jpg', '_crop.jpg')\n",
    "                    croppedImage.save(croppedOutputPath) #finally save augmented images\n",
    "                contrastAdjustedOutputPath = outputPath.replace('.png', '_bc.png').replace('.jpg', '_bc.jpg')\n",
    "\n",
    "                contrastAdjusted.save(contrastAdjustedOutputPath) # ^^\n",
    "\n",
    "label_augment_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwn5UcpOJIgo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwn5UcpOJIgo",
    "outputId": "c3e75fff-c3da-4c63-f928-6ba14a9b2674"
   },
   "outputs": [],
   "source": [
    "# Instantiate a dataset class\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, preprocessedFolder, vocab, transform=None):\n",
    "        self.data_dir = preprocessedFolder\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for root, _, files in os.walk(self.data_dir):\n",
    "          for file in files:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "              self.image_paths.append(os.path.join(root, file))\n",
    "              self.labels.append(os.path.basename(root))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      image_path = self.image_paths[idx]\n",
    "      label = self.labels[idx]\n",
    "      image = Image.open(image_path)\n",
    "\n",
    "      # transform (resizing and conversion to tensor)\n",
    "      if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "      # convert label into a tensor and pad it to the max length\n",
    "      label = torch.tensor([self.vocab[char] for char in label], dtype = torch.long)\n",
    "\n",
    "      padding_length = max_label_length - label.size(0)\n",
    "      if padding_length > 0:\n",
    "            label = torch.cat([label, torch.zeros(padding_length, dtype=torch.long)])\n",
    "\n",
    "      return image, label\n",
    "\n",
    "# Initialize folders and the datasets\n",
    "#preprocessedFolder = \"/content/preprocessed\"\n",
    "#nonprocessedFolder = \"/content/labeledImages\"\n",
    "preprocessedFolder = \"/Users/hufen/Coding/ECS 174/Project/preprocessed\"\n",
    "nonprocessedFolder = \"/Users/hufen/Coding/ECS 174/Project/labeledImages\"\n",
    "\n",
    "# Manually add in characters that were sanitized from preprocessing\n",
    "vocab_set.add('_')\n",
    "vocab_set.add('*')\n",
    "vocab_set.add('<')\n",
    "vocab_set.add('>')\n",
    "vocab_set.add(':')\n",
    "vocab_set.add('/')\n",
    "vocab_set.add('\\\\')\n",
    "vocab_set.add('?')\n",
    "vocab_set.add('\"')\n",
    "\n",
    "\n",
    "# Make a dictionary for vocab\n",
    "vocab = {char: idx for idx, char in enumerate(sorted(vocab_set))}\n",
    "vocab_size = len(vocab) + 1\n",
    "output_size = vocab_size\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((32, 128)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = HandwritingDataset(preprocessedFolder, vocab, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=True, num_workers = 0, pin_memory=True)\n",
    "\n",
    "testset = HandwritingDataset(nonprocessedFolder, vocab, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle=False, num_workers = 0, pin_memory=True)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(vocab)\n",
    "print(f\"Number of training samples: {len(trainloader.dataset)}\")\n",
    "print(f\"Number of test samples: {len(testloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfabf60-9fb9-412b-92d3-d937805b0d42",
   "metadata": {
    "id": "9cfabf60-9fb9-412b-92d3-d937805b0d42"
   },
   "outputs": [],
   "source": [
    "# cnn\n",
    "class testCNN(nn.Module):\n",
    "  def __init__(self, in_channels: int, out_channels: int):\n",
    "    super(testCNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    self.conv2 = nn.Conv2d(out_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "    self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn5 = nn.BatchNorm2d(512)\n",
    "\n",
    "    self.conv6 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn6 = nn.BatchNorm2d(512)\n",
    "\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # could change droupout\n",
    "    self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "    x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "    x = self.dropout(x)\n",
    "    x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "    x = self.dropout(x)\n",
    "    x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "    x = self.dropout(x)\n",
    "    x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "    x = self.dropout(x)\n",
    "    x = F.relu(self.bn6(self.conv6(x)))\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cb2a1-5207-4b91-aab9-1ff0e7380600",
   "metadata": {
    "id": "a00cb2a1-5207-4b91-aab9-1ff0e7380600"
   },
   "outputs": [],
   "source": [
    "# 2 LSTM layers\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        # cnn layers\n",
    "        self.cnnLayer = testCNN(1, 32)\n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 2, bias = True, batch_first = False, dropout = dropout_rate, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnnLayer(x)\n",
    "        # reshape output from cnn\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.permute(3, 0, 2, 1).contiguous()\n",
    "        x = x.view(width, batch_size, -1)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim = 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29684f-de2e-42b1-9416-a736bebebd21",
   "metadata": {
    "id": "3a29684f-de2e-42b1-9416-a736bebebd21"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizers\n",
    "model = BidirectionalLSTM(input_size = input_size, hidden_size = hidden_size, output_size = output_size, dropout_rate = dropout_rate)\n",
    "criterion = nn.CTCLoss(blank = 0, zero_infinity = True)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c20e1b-888a-4108-91b6-984d51ab2212",
   "metadata": {
    "id": "18c20e1b-888a-4108-91b6-984d51ab2212"
   },
   "outputs": [],
   "source": [
    "# Lists to store metrics for plotting\n",
    "loss_metric = []\n",
    "training_accuracy = []\n",
    "test_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JHi7Kmp-5PhX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JHi7Kmp-5PhX",
    "outputId": "3d76ecd4-d25e-4eb8-f62a-bd480cc43c96"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "t0 = time.time()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Statistics\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    skipped = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # introduce input and target lengths for ctc loss\n",
    "        input_lengths = torch.full((outputs.size(1),), outputs.size(0), dtype=torch.long, device=device)\n",
    "        target_lengths = torch.tensor([len(label[label > 0]) for label in labels], dtype=torch.long, device=device)\n",
    "        flattened_labels = labels[labels > 0].view(-1).to(device)\n",
    "\n",
    "        loss = criterion(outputs, flattened_labels, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics and calculate accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Decode predictions\n",
    "        _, predicted = outputs.max(2)\n",
    "        predicted = predicted.transpose(0, 1)\n",
    "\n",
    "        # Remove blanks from predictions\n",
    "        decoded_preds = []\n",
    "        for seq in predicted:\n",
    "            decoded_seq = []\n",
    "            prev_char = None\n",
    "            for char in seq:\n",
    "                if char != prev_char:\n",
    "                    decoded_seq.append(char.item())\n",
    "                    prev_char = char\n",
    "            decoded_preds.append(decoded_seq)\n",
    "\n",
    "        for pred, label in zip(decoded_preds, labels):\n",
    "            if pred == label[label > 0].tolist():\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if i % 400 == 399:    # print every 400 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 400:.3f}')\n",
    "            print(f'training accuracy: {correct / total * 100:.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Record the training accuracy and loss\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_accuracy = correct / total\n",
    "    loss_metric.append(epoch_loss)\n",
    "    training_accuracy.append(epoch_accuracy)\n",
    "\n",
    "    # Record the testing accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                _, predicted = outputs.max(2)\n",
    "                predicted = predicted.transpose(0, 1)\n",
    "\n",
    "                decoded_preds = []\n",
    "                for seq in predicted:\n",
    "                    decoded_seq = []\n",
    "                    prev_char = None\n",
    "                    for char in seq:\n",
    "                        if char != prev_char:\n",
    "                            decoded_seq.append(char.item())\n",
    "                        prev_char = char\n",
    "                    decoded_preds.append(decoded_seq)\n",
    "\n",
    "                for pred, label in zip(decoded_preds, labels):\n",
    "                    if pred == label[label > 0].tolist():\n",
    "                        correct += 1\n",
    "                total += labels.size(0)\n",
    "        except(OSError, Image.UnidentifiedImageError) as e:\n",
    "            skipped += 1\n",
    "\n",
    "\n",
    "    acc = correct / total\n",
    "    test_accuracy.append(acc)\n",
    "    print(f'Testing Accuracy: {acc * 100:.2f}%')\n",
    "    print(f'Skipped Images: {skipped}')\n",
    "\n",
    "    t1 = time.time()\n",
    "    total_time = t1-t0\n",
    "    print(f\"Training and Testing took {total_time:.2f} seconds until now\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ovJAoSkAAxla",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ovJAoSkAAxla",
    "outputId": "a2acb5c6-7f85-4758-ebfc-4c7da48af9ab"
   },
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(loss_metric, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_accuracy, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(test_accuracy, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IsaVnzwARWhc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsaVnzwARWhc",
    "outputId": "a0eec389-799a-44d5-bde0-2ae440fbcafe"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = outputs.max(2)\n",
    "            predicted = predicted.transpose(0, 1)\n",
    "\n",
    "            decoded_preds = []\n",
    "            for seq in predicted:\n",
    "                decoded_seq = []\n",
    "                prev_char = None\n",
    "                for char in seq:\n",
    "                    if char != prev_char:\n",
    "                        decoded_seq.append(char.item())\n",
    "                    prev_char = char\n",
    "                decoded_preds.append(decoded_seq)\n",
    "\n",
    "            for pred, label in zip(decoded_preds, labels):\n",
    "                if pred == label[label > 0].tolist():\n",
    "                    correct += 1\n",
    "            total += labels.size(0)\n",
    "\n",
    "    except(OSError, Image.UnidentifiedImageError) as e:\n",
    "        print(f\"Skipping image due to unidentifies image: {e}\")\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VR2JyDW-ibab",
   "metadata": {
    "id": "VR2JyDW-ibab"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model, '/Users/hufen/Coding/ECS 174/Project/model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yo6R9Moq9TLZ",
   "metadata": {
    "id": "Yo6R9Moq9TLZ"
   },
   "outputs": [],
   "source": [
    "# Using the model\n",
    "def gray_scale_testing(inputPath):\n",
    "  try:\n",
    "    image = Image.open(inputPath)\n",
    "    if image.mode == \"RGB\":\n",
    "        pixels = image.load()\n",
    "        width, height = image.size\n",
    "        grayscaleImage = Image.new(\"L\", (width, height))\n",
    "        grayscalePixels = grayscaleImage.load()\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                r, g, b = pixels[x, y]\n",
    "                grayValue = int(0.299 * r + 0.587 * g + 0.114 * b)\n",
    "                grayscalePixels[x, y] = grayValue\n",
    "        return grayscaleImage\n",
    "    else:\n",
    "        return image\n",
    "  except(OSError, Image.UnidentifiedImageError) as e:\n",
    "      print(f\"Image processing error, return none\")\n",
    "      return None\n",
    "\n",
    "model = torch.load('/Users/hufen/Coding/ECS 174/Project/model/model.pth')\n",
    "\n",
    "image_path = '/path/to/image.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "grayImage = gray_scale_testing(image)\n",
    "\n",
    "processed_image = transform(grayImage).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "  output = model(processed_image)\n",
    "\n",
    "_, predicted = outputs.max(2)\n",
    "predicted = predicted.transpose(0, 1)\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(image)  # Show the original (unprocessed) image\n",
    "plt.title(f\"Predicted: {predicted}\", fontsize=16)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
